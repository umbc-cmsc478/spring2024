---
layout: page
title: Exams
nav_exclude: false
# permalink: /:path/
---

This page will be updated shortly before the midterm and final exams to reflect what we actually covered this semester.

# Midterm exam

## <span style="color:blue">Reading Guide: </span>
Listed below are the minimum things you should be prepared to do. This is not an all-inclusive list, but you should at least be prepared to do these things:

### Logistic Regression, Linear Regression
- What is the cost function and what is the log-likelihood?
- How do you obtain the gradient descent update rule from cost function?
- How do you get to log-likelihood from h(x)?
- Why do we need 0-1 and perceptron loss?
- What is minimizing least squares?

### Multiclass classification
- How is Multiclass classification connected to Logistic Regression (the general idea)
- what is their loss (how do you train multiclass)? 

### Naive Bayes (NB)
- How to estimate parameters for likelihood functions using bayes rule? What parameters needed to be calculated to obtain 
P(Y|X)?
- How do you handle continuous and discrete X in NB?
- How many independent parameters we need to estimate for calculation of joint probabilities?
- how does NB assumption improves it?
- what are the subtleties of Naive Bayes?
- How to get log-likelihood from P(D|theta) and how get MLE and MAP estimate of theta from it?

<!-- ### Generative vs discriminative
- how many parameters needed for generative classifiers, 
- how does NB assumption improves it,
- how do you calculate conditional probabilities from joint probabilities? -->


<!-- ### Kernel
    Stanford lecture notes (SML) 5.1, 5.2, 5.4 (page 53, 54, 55)

- Focus on What kinds of functions K(·, ·) can correspond to some feature map φ?
- How to calculate φ(x) from x?
- How to calculate the weight-parameters (w/theta) given the decision function? You will find examples in the sample exams

### SVM
- Impact of offset, impact of C and slack variable (slides show examples) -->

### Bias - variance, and Cross-validation
- How is model complexity connected to bias, variance, and test error?
- How does L1 and L2 regularization affect classifiers?
- How does cross-validation help us to better generalize?

### Decision Tree
- Run Simulation of a decision tree
- When does overfitting happen? How to avoid overfitting in decision tree?
- How does the decision boundary look like?

### PCA
- What are principal components? How do you find them?
- How to get reduced dimension?
- How do you #components?

### K-means, KNN
- What are the problems of KNN? How do you solve them?
- What is hierarchical clustering? What does the evaluation metrics evaluate in clusters?
- KNN decision boundaries
- How variance changes with neighbor increase in KNN?
- Advantage and Disadvantage


## <span style="color:blue">Sample Questions </span>
I am providing a sample exam. Please be reminded that not all the topics in the questions have been convered in our classes. You can ignore them.

- [sample-question](assets/exams/sample-midterm.pdf)



<!-- -->

<!-- * Properties of Kernel -->


